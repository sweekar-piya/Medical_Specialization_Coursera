{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this week on the course, we will be learning about _interpretability_ of Machine learning models that we have built in course 1 and 2.\n",
    "\n",
    "In the first part of this week, we will be diving in _feature importance methods_ for explaining AI models. Such method quantifies the importance of feature on global level, instead of per data sample level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisities\n",
    "- Learning course 1 and 2, as most of the models used in this week is imported from those course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What will be covered?\n",
    "\n",
    "Within feature importance method, the course will be covering:\n",
    "\n",
    "1. Drop Column Method\n",
    "2. Permutation Method\n",
    "\n",
    "Feature Importance methods are used for problems where structured tabular dataset are used, in which features are represented as column within the table. Feature importance methods strive to compute the contribution of each of the feature to the performance of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Column Method\n",
    "\n",
    "Drop column methods, as the name suggest, drop all the column except one, and computes the model performace.\n",
    "Suppose to develop a prognostic model, we use `Blood Pressure` and `Age` as feature to obtain the `risk of death`. Here are the steps to compute feature importance using this method:\n",
    "1. First, train the model on all the features, and subsequent performance is computed on test set.\n",
    "\n",
    "2. Next, iteratively, create the prognostic model with single feature from pool of features and compute performance on test set.\n",
    "\n",
    "    <figure>\n",
    "    <center><img src=\"../../assets/W3/W3_P1_drop_column_method.png\" width=\"900\">\n",
    "    <figcaption align=\"center\"> Fig 1: Drop Column method visualization </figcaption>\n",
    "    </figure>\n",
    "\n",
    "3. Finally, compute the importance of each feature by subtracting performance (say, C-Index) of original model with iteratively generated single feature models. (Higher score means more importance.) In order to compute the importance scores, the formula will be as:\n",
    "\n",
    "    | Feature | Importance|\n",
    "    | --- | --- |\n",
    "    | Age | $\\text{perf(\\{BP, Age\\})-perf(\\{BP\\})} $|\n",
    "    | BP  | $\\text{perf(\\{BP, Age\\})-perf(\\{Age\\})} $|\n",
    "\n",
    "### Disadvantage\n",
    "- As $n$ additional models are created for $n$ features, so this method is computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Method\n",
    "\n",
    "Permutation method fixes the problem of training multiple models in drop column method. Instead of using multiple model, we evaluate a single model on different permutation of feature set - shuffled feature.\n",
    "\n",
    "Returning to the previous example of prognostic model, here are the steps to compute permutation feature importance:\n",
    "1. First, train the model on the entire feature set and compute the performance on test set.\n",
    "2. Next, shuffle one column at a time of the test set and compute the performance on same model. Iteratively conduct this steps for all the features.\n",
    "\n",
    "    <figure>\n",
    "    <center><img src=\"../../assets/W3/W3_P1_permutation_method.png\" width=\"900\">\n",
    "    <figcaption align=\"center\"> Fig 2: Permutation method visualization </figcaption>\n",
    "    </figure>\n",
    "\n",
    "3. Finally, compute the permutation feature importance values by using the following formula:\n",
    "\n",
    "    | Feature | Importance|\n",
    "    | --- | --- |\n",
    "    | Age | $\\text{perf(\\{BP, Age\\})-perf(\\{BP, Shuffled Age\\})} $|\n",
    "    | BP  | $\\text{perf(\\{BP, Age\\})-perf(\\{Shuffled BP, Age\\})} $|\n",
    "\n",
    "    Here, higher value mean more importance because when we subtract the two evaluation metrics we are capturing the change in performance due to shuffled feature. Higher degradation in performance when a feature is shuffled means that the feature is of higher importance.\n",
    "\n",
    "Note: Generally, each feature shuffling evaluation is done multiple times (look at the lab for this section W3_L1) . Finally their average is taken to compute the performance of the model with shuffled feature.\n",
    "\n",
    "That's all for this lecture. In the next section, we will be learning about \"Individual Feature Importance\" Methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
