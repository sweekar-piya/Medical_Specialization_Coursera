{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In the previous lecture and Lab, we developed an Automatic Label Extraction model to extract labels from radiology reports. In this lecture, we will be looking on assessing those models using various evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the steps of evaluating the model?\n",
    "\n",
    "As with typical ML (supervised) problem, we require a set of ground truth (actual) labels and compare them with predicted labels for to assess how our model is performing. \n",
    "\n",
    "But how do we get the actual labels? -> Here we require aid from radiologist (or experts in other cases.). We can either, assign radiologist to look at the _report summary or the image_ and set the actual labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "The following evaluation metrics are used to assess the metrics:\n",
    "\n",
    "1. Precision (Positive Predictive Value) = $\\frac{\\text{TP}}{\\text{TP+FP}}$\n",
    "\n",
    "2. Recall (Sensitivity) = $\\frac{\\text{TP}}{\\text{TP+FN}}$\n",
    "\n",
    "3. F1-Score (Dice Coefficient Score) = $\\frac{2*\\text{Precision}*\\text{Recall}}{\\text{Precision}+\\text{Recall}}$\n",
    "\n",
    "Since there can be multiple labels per automatic label extraction task, to compute the metrics:\n",
    "1. either, we first compute evaluation metrics per label and take the average, known as macro-average.\n",
    "2. or, compute the metrics global, known as micro-average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
